# Mask Detection using CNNs (Part B)

## Table of Contents
- [Mask Detection using CNNs (Part B)](#mask-detection-using-cnns-part-b)
  - [Table of Contents](#table-of-contents)
  - [1. Introduction](#1-introduction)
  - [2. Dataset \& Preprocessing](#2-dataset--preprocessing)
  - [3. Normal Analysis](#3-normal-analysis)
    - [3.1 Model Architecture](#31-model-architecture)
    - [3.2 Training Process](#32-training-process)
    - [3.3 Evaluation](#33-evaluation)
  - [4. Advanced Analysis](#4-advanced-analysis)
    - [4.1 Motivation for Advanced Analysis](#41-motivation-for-advanced-analysis)
    - [4.2 Model Architectures](#42-model-architectures)
    - [4.3 Training Process](#43-training-process)
    - [4.4 Evaluation](#44-evaluation)
  - [5. Differences Between Models](#5-differences-between-models)
    - [5.1 Model Complexity \& Architecture](#51-model-complexity--architecture)
    - [5.2 Computational Efficiency](#52-computational-efficiency)
    - [5.3 Performance \& Accuracy](#53-performance--accuracy)
    - [5.4 Takeaways](#54-takeaways)
  - [6. Model Performance Visualization](#6-model-performance-visualization)
    - [6.1 Normal Analysis Model Accuracy Plots](#61-normal-analysis-model-accuracy-plots)
      - [ğŸ“Œ Accuracy Trends for Normal Analysis](#-accuracy-trends-for-normal-analysis)
    - [6.2 Advanced Analysis Model Accuracy Plots](#62-advanced-analysis-model-accuracy-plots)
      - [ğŸ“Œ Accuracy Trends for Advanced Analysis](#-accuracy-trends-for-advanced-analysis)
    - [6.3 Comparative Observations](#63-comparative-observations)
      - [ğŸ“Œ Conclusion: **More complex doesnâ€™t always mean better.**](#-conclusion-more-complex-doesnt-always-mean-better)

---

## 1. Introduction

This project aims to classify images of faces as **"with mask"** or **"without mask"** using **Convolutional Neural Networks (CNNs)**. In **Part A**, we used **feature extraction with multiple machine learning models** (SVM, MLP, XGBoost, etc.), whereas in **Part B**, we focus on **deep learning-based classification** using CNNs.

We divide Part B into **two phases**:
- **Normal Analysis**: Uses smaller CNN architectures with different activation functions and optimizers.
- **Advanced Analysis**: Uses larger CNN models (including ResNet-like and MobileNet).

The goal is to **compare the effectiveness of simple vs. advanced CNN architectures** for mask detection.
**Directory Structure**
```
C:.
â”‚   B_README.MD
â”‚
â”œâ”€â”€â”€Advanced_Analysis
â”‚   â”‚   Colab_USAGE_CNN.ipynb
â”‚   â”‚   main.ipynb
â”‚   â”‚
â”‚   â””â”€â”€â”€snapshots
â”‚       â”‚   hyperparams.json
â”‚       â”‚
â”‚       â”œâ”€â”€â”€histories
â”‚       â”‚       baseline_cnn.json
â”‚       â”‚       mobilenet.json
â”‚       â”‚       resnet_like.json
â”‚       â”‚       vgg_like.json
â”‚       â”‚
â”‚       â”œâ”€â”€â”€models
â”‚       â”‚       baseline_cnn.h5
â”‚       â”‚       mobilenet.h5
â”‚       â”‚       resnet_like.h5
â”‚       â”‚       vgg_like.h5
â”‚       â”‚
â”‚       â””â”€â”€â”€plots
â”‚               baseline_cnn.png
â”‚               mobilenet.png
â”‚               resnet_like.png
â”‚               vgg_like.png
â”‚
â””â”€â”€â”€Normal_Analysis
    â”‚   main.ipynb
    â”‚
    â”œâ”€â”€â”€cnn_models
    â”‚       history_relu_adam.json
    â”‚       history_relu_sgd.json
    â”‚       history_tanh_adam.json
    â”‚       model_relu_adam.keras
    â”‚       model_relu_sgd.keras
    â”‚       model_tanh_adam.keras
    â”‚
    â””â”€â”€â”€cnn_processed_data
            relu_adam.png
            relu_sgd.png
            tanh_adam.png
            X.npy
            y.npy
```
---

## 2. Dataset & Preprocessing

The dataset is preprocessed into **NumPy arrays** (`X.npy`, `y.npy`) for efficient loading and training. These files are used across both Normal and Advanced Analyses.

- **Image Size**: `(128, 128, 3)`
- **Classes**: `with_mask (1)` and `without_mask (0)`
- **Train-Test Split**: `80%-20%`

---

## 3. Normal Analysis

The **Normal Analysis** focuses on simple CNN models trained directly in **Jupyter Notebook**. The purpose is to test the effectiveness of CNNs using basic architectures before moving to advanced models.

### 3.1 Model Architecture

Three models were trained with varying activation functions and optimizers:
- **Model 1:** `ReLU + Adam`
- **Model 2:** `Tanh + Adam`
- **Model 3:** `ReLU + SGD`

All models follow a **3-layer CNN architecture**:
1. **Conv2D(32) â†’ MaxPooling**
2. **Conv2D(64) â†’ MaxPooling**
3. **Conv2D(128) â†’ MaxPooling â†’ Flatten â†’ Dense(128) â†’ Output Layer**

### 3.2 Training Process

- **Epochs**: `10`
- **Batch Size**: `32`
- **Loss Function**: `Binary Crossentropy`
- **Evaluation Metric**: `Accuracy`

### 3.3 Evaluation

| Model | Validation Accuracy |
|--------|---------------------|
| `ReLU + Adam`  | **96.70%** |
| `Tanh + Adam`  | **96.58%** |
| `ReLU + SGD`   | **90.84%** |

**Key Findings:**
âœ… `ReLU + Adam` performed the best.
âš ï¸ `ReLU + SGD` performed the worst, likely due to **SGD's slow convergence**.

---

## 4. Advanced Analysis

### 4.1 Motivation for Advanced Analysis

Although **Normal Analysis** achieved high accuracy, its models were **simple** and might not generalize well on more diverse datasets. To improve, we explore **larger and more complex architectures** using **Google Colab (GPU acceleration)**.

### 4.2 Model Architectures

Four deep CNN models were trained:

1. **Baseline CNN** â€“ Similar to Normal Analysis but deeper.
2. **VGG-like CNN** â€“ Inspired by VGG architecture, uses **more layers**.
3. **ResNet-like CNN** â€“ Introduces **residual connections** to prevent vanishing gradients.
4. **MobileNet** â€“ Lightweight, designed for mobile applications.

### 4.3 Training Process

- **Epochs**: `25` (more than Normal Analysis for deeper models)
- **Batch Size**: `64` (higher for efficient GPU utilization)
- **Optimizer**: `Adam` for all models

### 4.4 Evaluation

| Model | Validation Accuracy | Validation Loss |
|---------------|---------------------|-----------------|
| **Baseline CNN** | **97.68%** | `0.1619` |
| **VGG-like CNN** | `96.09%` | `0.2767` |
| **ResNet-like CNN** | `95.48%` | `0.2748` |
| **MobileNet** | ğŸš¨ `43.83%` | ğŸš¨ `2.3555` |

---

## 5. Differences Between Models

The four CNN models used in Advanced Analysisâ€”**Baseline CNN, VGG-like, ResNet-like, and MobileNet**â€”vary significantly in their architectures, complexity, and intended purpose. Below is a structured comparison highlighting their differences:

### 5.1 Model Complexity & Architecture

| Model | Depth | Key Features | Intended Benefit |
|--------------|------|--------------------------------|----------------|
| **Baseline CNN** | Shallow | 3 Conv layers, MaxPooling | Simple, fast training |
| **VGG-like CNN** | Deeper | 5 Conv layers, deeper feature extraction | Stronger feature learning |
| **ResNet-like CNN** | Deep with Residuals | Skip connections to prevent vanishing gradients | Improved gradient flow |
| **MobileNet** | Lightweight | Depthwise separable convolutions | Optimized for mobile devices |

- **Baseline CNN** is the simplest and follows a traditional architecture.
- **VGG-like CNN** is inspired by VGG networks and adds depth to improve feature extraction.
- **ResNet-like CNN** introduces **residual connections**, allowing deeper networks to train efficiently.
- **MobileNet** uses **depthwise separable convolutions** to make the model smaller and faster.

### 5.2 Computational Efficiency

| Model | Parameters | Training Time (Approx) | GPU Requirement |
|--------------|------------|------------------|-----------------|
| **Baseline CNN** | **~1.2M** | ğŸš€ Fastest | Low |
| **VGG-like CNN** | **~3.5M** | ğŸ¢ Slower | Medium |
| **ResNet-like CNN** | **~2.9M** | âš–ï¸ Moderate | High |
| **MobileNet** | **~2.2M** | â³ Slow (unexpected) | Low (but failed) |

- **Baseline CNN** is computationally light and trains quickly.
- **VGG-like CNN** has more layers and parameters, increasing training time.
- **ResNet-like CNN** is deep but efficient due to residual connections.
- **MobileNet** should have been fast, but its training failed, possibly due to missing **pretrained weights**.

### 5.3 Performance & Accuracy

| Model | Validation Accuracy | Key Issue |
|--------------|----------------|----------------------|
| **Baseline CNN** | **97.68%** âœ… | Strong performance |
| **VGG-like CNN** | **96.09%** ğŸ”» | Overfitting risk |
| **ResNet-like CNN** | **95.48%** ğŸ”» | Similar to VGG-like |
| **MobileNet** | ğŸš¨ **43.83%** âŒ | Training failed (likely weight issue) |

- **Baseline CNN performed the best**, proving that even a simple model can achieve high accuracy.
- **VGG-like and ResNet-like models did not significantly improve accuracy**, suggesting **diminishing returns with depth**.
- **MobileNet completely failed**, likely because it lacked pretrained weights, making it unable to extract meaningful features.

### 5.4 Takeaways

âœ… **Simple CNNs can be very effective** â€“ Baseline CNN outperformed deeper models.
âš ï¸ **Depth doesnâ€™t always mean better performance** â€“ Overfitting can be an issue in deeper networks.
âŒ **Pretrained weights matter** â€“ MobileNet needs them for effective feature extraction.

---

## 6. Model Performance Visualization

To better understand how each model performed, we visualize their training history. Accuracy plots for both **Normal Analysis** and **Advanced Analysis** provide insights into learning trends, overfitting, and generalization capabilities.

### 6.1 Normal Analysis Model Accuracy Plots

The Normal Analysis involved three models trained with different activation function and optimizer combinations:

- **ReLU + Adam**
- **ReLU + SGD**
- **Tanh + Adam**

#### ğŸ“Œ Accuracy Trends for Normal Analysis

Stored in `cnn_processed_data/` as:

- ![ReLU + Adam](/Project_1/src/binary_classification_task/B_Binary_Classification_Using_CNN/Normal_Analysis/cnn_processed_data/relu_adam.png)
- ![ReLU + SGD](/Project_1/src/binary_classification_task/B_Binary_Classification_Using_CNN/Normal_Analysis/cnn_processed_data/relu_sgd.png)
- ![Tanh + Adam](/Project_1/src/binary_classification_task/B_Binary_Classification_Using_CNN/Normal_Analysis/cnn_processed_data/tanh_adam.png)

These plots highlight:

âœ… **ReLU + Adam achieved stable convergence** with minimal overfitting.
ğŸ”» **ReLU + SGD had fluctuations**, indicating some instability.
âš ï¸ **Tanh + Adam showed slower learning** but was stable.

### 6.2 Advanced Analysis Model Accuracy Plots

The Advanced Analysis trained four distinct CNN architectures:

- **Baseline CNN**
- **VGG-like CNN**
- **ResNet-like CNN**
- **MobileNet**

#### ğŸ“Œ Accuracy Trends for Advanced Analysis

Stored in `snapshots/plots/` as:

| Baseline CNN | VGG-like |
|-------------|---------|
| ![Baseline CNN](/Project_1/src/binary_classification_task/B_Binary_Classification_Using_CNN/Advanced_Analysis/snapshots/plots/baseline_cnn.png) | ![VGG-like](/Project_1/src/binary_classification_task/B_Binary_Classification_Using_CNN/Advanced_Analysis/snapshots/plots/vgg_like.png) |

| ResNet-like | MobileNet |
|------------|---------|
| ![ResNet-like](/Project_1/src/binary_classification_task/B_Binary_Classification_Using_CNN/Advanced_Analysis/snapshots/plots/resnet_like.png) | ![MobileNet](/Project_1/src/binary_classification_task/B_Binary_Classification_Using_CNN/Advanced_Analysis/snapshots/plots/mobilenet.png) |
These plots reveal:

âœ… **Baseline CNN maintained strong performance** throughout training.
âš ï¸ **VGG-like and ResNet-like models overfitted slightly** compared to the baseline.
âŒ **MobileNet failed to learn** due to missing pretrained weights, causing it to collapse.

### 6.3 Comparative Observations

| Model Type | Best Performing Model | Overfitting Risk | Key Observation |
|------------|---------------------|-----------------|-----------------|
| **Normal Analysis** | ReLU + Adam | Low | Simpler networks generalize well. |
| **Advanced Analysis** | Baseline CNN | Low | Deeper networks didn't provide significant gains. |

#### ğŸ“Œ Conclusion: **More complex doesnâ€™t always mean better.**
- The **Baseline CNN (Advanced)** outperformed deeper models like **VGG-like and ResNet-like**.
- The **simpler models in Normal Analysis performed nearly as well as Advanced Analysis models**.
- **MobileNet failed**, reinforcing the importance of **pretrained weights** in transfer learning approaches.

---